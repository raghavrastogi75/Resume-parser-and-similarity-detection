{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c0CXkXJB3DI"
      },
      "source": [
        "# Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0THFbHWL6ej7"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_F-j6x49uBY",
        "outputId": "658c22da-74f4-4b69-a0ae-feab5ce21e75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/raghavrastogi75/Resume-parser-and-similarity-detection.git"
      ],
      "metadata": {
        "id": "TTwoYbn5lQnw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95298a44-d005-4ca3-e27d-d657cfb23c85"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Resume-parser-and-similarity-detection'...\n",
            "remote: Enumerating objects: 47, done.\u001b[K\n",
            "remote: Counting objects: 100% (47/47), done.\u001b[K\n",
            "remote: Compressing objects: 100% (46/46), done.\u001b[K\n",
            "remote: Total 47 (delta 20), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (47/47), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dk38fFTCdYd"
      },
      "source": [
        "# Install Spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VloJGZkiCXrO",
        "outputId": "a99076d6-4e40-4d4f-fe3b-be3c53c06061"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spacy==2.1.4\n",
            "  Downloading spacy-2.1.4-cp37-cp37m-manylinux1_x86_64.whl (29.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 29.8 MB 128 kB/s \n",
            "\u001b[?25hRequirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.4) (0.9.1)\n",
            "Collecting plac<1.0.0,>=0.9.6\n",
            "  Downloading plac-0.9.6-py2.py3-none-any.whl (20 kB)\n",
            "Collecting preshed<2.1.0,>=2.0.1\n",
            "  Downloading preshed-2.0.1-cp37-cp37m-manylinux1_x86_64.whl (82 kB)\n",
            "\u001b[K     |████████████████████████████████| 82 kB 340 kB/s \n",
            "\u001b[?25hCollecting blis<0.3.0,>=0.2.2\n",
            "  Downloading blis-0.2.4-cp37-cp37m-manylinux1_x86_64.whl (3.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2 MB 33.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: srsly<1.1.0,>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.4) (1.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.4) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.4) (2.0.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.4) (1.0.6)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.4) (1.21.6)\n",
            "Collecting jsonschema<3.1.0,>=2.6.0\n",
            "  Downloading jsonschema-3.0.2-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 2.2 MB/s \n",
            "\u001b[?25hCollecting thinc<7.1.0,>=7.0.2\n",
            "  Downloading thinc-7.0.8-cp37-cp37m-manylinux1_x86_64.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 29.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from jsonschema<3.1.0,>=2.6.0->spacy==2.1.4) (57.4.0)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema<3.1.0,>=2.6.0->spacy==2.1.4) (1.15.0)\n",
            "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema<3.1.0,>=2.6.0->spacy==2.1.4) (0.18.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema<3.1.0,>=2.6.0->spacy==2.1.4) (21.4.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.4) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.4) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.4) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.4) (2021.10.8)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from thinc<7.1.0,>=7.0.2->spacy==2.1.4) (4.64.0)\n",
            "Installing collected packages: preshed, plac, blis, thinc, jsonschema, spacy\n",
            "  Attempting uninstall: preshed\n",
            "    Found existing installation: preshed 3.0.6\n",
            "    Uninstalling preshed-3.0.6:\n",
            "      Successfully uninstalled preshed-3.0.6\n",
            "  Attempting uninstall: plac\n",
            "    Found existing installation: plac 1.1.3\n",
            "    Uninstalling plac-1.1.3:\n",
            "      Successfully uninstalled plac-1.1.3\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 0.4.1\n",
            "    Uninstalling blis-0.4.1:\n",
            "      Successfully uninstalled blis-0.4.1\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Attempting uninstall: jsonschema\n",
            "    Found existing installation: jsonschema 4.3.3\n",
            "    Uninstalling jsonschema-4.3.3:\n",
            "      Successfully uninstalled jsonschema-4.3.3\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "nbclient 0.6.0 requires jupyter-client>=6.1.5, but you have jupyter-client 5.3.5 which is incompatible.\n",
            "en-core-web-sm 2.2.5 requires spacy>=2.2.2, but you have spacy 2.1.4 which is incompatible.\u001b[0m\n",
            "Successfully installed blis-0.2.4 jsonschema-3.0.2 plac-0.9.6 preshed-2.0.1 spacy-2.1.4 thinc-7.0.8\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy==2.1.4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtQ1NcmkCk-a"
      },
      "source": [
        "# Create functions to convert Json file to Spacy format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_LcWJvZOUnT7"
      },
      "outputs": [],
      "source": [
        "# import logging\n",
        "import json\n",
        "import re\n",
        "\n",
        "# JSON formatting functions\n",
        "def json_to_spacy(JSON_FilePath):\n",
        "    training_data = []\n",
        "    lines=[]\n",
        "\n",
        "    \n",
        "    with open(JSON_FilePath, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    #For each json line\n",
        "    for line in lines:\n",
        "        data = json.loads(line)\n",
        "\n",
        "        #add content key in data dictionary\n",
        "        text = data['content'].replace(\"\\n\", \" \")\n",
        "        entities = []\n",
        "\n",
        "        #add annotation key in dictionary\n",
        "        data_annotations = data['annotation']\n",
        "        if data_annotations is not None:\n",
        "            for annotation in data_annotations:\n",
        "                #only a single point in text annotation.\n",
        "                point = annotation['points'][0]\n",
        "                labels = annotation['label']\n",
        "                # handle both list of labels or a single label.\n",
        "                if not isinstance(labels, list):\n",
        "                    labels = [labels]\n",
        "\n",
        "                for label in labels:\n",
        "                    p_start = point['start']\n",
        "                    p_end = point['end']\n",
        "                    p_text = point['text']\n",
        "\n",
        "                    #find the left and right white spaces and remove them\n",
        "                    lstrip_diff = len(p_text) - len(p_text.lstrip())\n",
        "                    rstrip_diff = len(p_text) - len(p_text.rstrip())\n",
        "\n",
        "                    #move the pointer for white spaces\n",
        "                    if lstrip_diff != 0:\n",
        "                        p_start = p_start + lstrip_diff\n",
        "                    if rstrip_diff != 0:\n",
        "                        p_end = p_end - rstrip_diff\n",
        "\n",
        "                    #add the updates locations of the entities\n",
        "                    entities.append((p_start, p_end + 1 , label))\n",
        "        training_data.append((text, {\"entities\" : entities}))\n",
        "    return training_data\n",
        "\n",
        "def trim_entity_spans(data: list) -> list:\n",
        "    #Removes leading and trailing white spaces from entity spans.\n",
        "    #Returns The cleaned data.\n",
        "\n",
        "    inval_span_tokens = re.compile(r'\\s')\n",
        "\n",
        "    tidy_data = []\n",
        "    for text, annotations in data:\n",
        "        entities = annotations['entities']\n",
        "        val_entities = []\n",
        "        for start, end, label in entities:\n",
        "            val_start = start\n",
        "            val_end = end\n",
        "\n",
        "            # remove the whitespaces in the entity spans\n",
        "            while val_start < len(text) and inval_span_tokens.match(\n",
        "                    text[val_start]):\n",
        "                val_start += 1\n",
        "            while val_end > 1 and inval_span_tokens.match(\n",
        "                    text[val_end - 1]):\n",
        "                val_end -= 1\n",
        "            val_entities.append([val_start, val_end, label])\n",
        "        tidy_data.append([text, {'entities': val_entities}])\n",
        "    return tidy_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rU9TneQ5C2NJ"
      },
      "source": [
        "# Apply above conversion and cleaning on the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrUVlKloU2o9",
        "outputId": "159abc49-11d1-4bcf-83cd-aa7fc2786924"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Abhishek Jha Application Development Associate - Accenture  Bengaluru, Karnataka - Email me on Indeed: indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a  • To work for an organization which provides me the opportunity to improve my skills and knowledge for my individual and company's growth in best possible ways.  Willing to relocate to: Bangalore, Karnataka  WORK EXPERIENCE  Application Development Associate  Accenture -  November 2017 to Present  Role: Currently working on Chat-bot. Developing Backend Oracle PeopleSoft Queries for the Bot which will be triggered based on given input. Also, Training the bot for different possible utterances (Both positive and negative), which will be given as input by the user.  EDUCATION  B.E in Information science and engineering  B.v.b college of engineering and technology -  Hubli, Karnataka  August 2013 to June 2017  12th in Mathematics  Woodbine modern school  April 2011 to March 2013  10th  Kendriya Vidyalaya  April 2001 to March 2011  SKILLS  C (Less than 1 year), Database (Less than 1 year), Database Management (Less than 1 year), Database Management System (Less than 1 year), Java (Less than 1 year)  ADDITIONAL INFORMATION  Technical Skills  https://www.indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a?isid=rex-download&ikw=download-top&co=IN   • Programming language: C, C++, Java • Oracle PeopleSoft • Internet Of Things • Machine Learning • Database Management System • Computer Networks • Operating System worked on: Linux, Windows, Mac  Non - Technical Skills  • Honest and Hard-Working • Tolerant and Flexible to Different Situations • Polite and Calm • Team-Player\",\n",
              " {'entities': [[1296, 1622, 'Skills'],\n",
              "   [993, 1154, 'Skills'],\n",
              "   [939, 957, 'College Name'],\n",
              "   [883, 905, 'College Name'],\n",
              "   [856, 860, 'Graduation Year'],\n",
              "   [771, 814, 'College Name'],\n",
              "   [727, 769, 'Designation'],\n",
              "   [407, 416, 'Companies worked at'],\n",
              "   [372, 405, 'Designation'],\n",
              "   [95, 145, 'Email Address'],\n",
              "   [60, 69, 'Location'],\n",
              "   [49, 58, 'Companies worked at'],\n",
              "   [13, 46, 'Designation'],\n",
              "   [0, 12, 'Name']]}]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "data = trim_entity_spans(json_to_spacy(\"/content/Resume-parser-and-similarity-detection/Entity Recognition in Resumes.json\"))\n",
        "\n",
        "data[0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qS_NLNVDUsE"
      },
      "source": [
        "# Cleaning the Data - Remove overlapping entities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjfnbcjaZEtt",
        "outputId": "d8c3dd2d-5217-4cbb-d11d-c2f9988888bb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(\"Abhishek Jha Application Development Associate - Accenture  Bengaluru, Karnataka - Email me on Indeed: indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a  • To work for an organization which provides me the opportunity to improve my skills and knowledge for my individual and company's growth in best possible ways.  Willing to relocate to: Bangalore, Karnataka  WORK EXPERIENCE  Application Development Associate  Accenture -  November 2017 to Present  Role: Currently working on Chat-bot. Developing Backend Oracle PeopleSoft Queries for the Bot which will be triggered based on given input. Also, Training the bot for different possible utterances (Both positive and negative), which will be given as input by the user.  EDUCATION  B.E in Information science and engineering  B.v.b college of engineering and technology -  Hubli, Karnataka  August 2013 to June 2017  12th in Mathematics  Woodbine modern school  April 2011 to March 2013  10th  Kendriya Vidyalaya  April 2001 to March 2011  SKILLS  C (Less than 1 year), Database (Less than 1 year), Database Management (Less than 1 year), Database Management System (Less than 1 year), Java (Less than 1 year)  ADDITIONAL INFORMATION  Technical Skills  https://www.indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a?isid=rex-download&ikw=download-top&co=IN   • Programming language: C, C++, Java • Oracle PeopleSoft • Internet Of Things • Machine Learning • Database Management System • Computer Networks • Operating System worked on: Linux, Windows, Mac  Non - Technical Skills  • Honest and Hard-Working • Tolerant and Flexible to Different Situations • Polite and Calm • Team-Player\",\n",
              " {'entities': [[1296, 1622, 'Skills'],\n",
              "   [993, 1154, 'Skills'],\n",
              "   [939, 957, 'College Name'],\n",
              "   [883, 905, 'College Name'],\n",
              "   [856, 860, 'Graduation Year'],\n",
              "   [771, 814, 'College Name'],\n",
              "   [727, 769, 'Designation'],\n",
              "   [407, 416, 'Companies worked at'],\n",
              "   [372, 405, 'Designation'],\n",
              "   [95, 145, 'Email Address'],\n",
              "   [60, 69, 'Location'],\n",
              "   [49, 58, 'Companies worked at'],\n",
              "   [13, 46, 'Designation'],\n",
              "   [0, 12, 'Name']]})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "def clean_ents(training_data):\n",
        "    \n",
        "    clean_data = []\n",
        "    for text, annotation in training_data:\n",
        "        \n",
        "        ents = annotation.get('entities')\n",
        "        ents_copy = ents.copy()\n",
        "        \n",
        "        # append ent only if it is longer than its overlapping ent\n",
        "        i = 0\n",
        "        for ent in ents_copy:\n",
        "            j = 0\n",
        "            for overlapping_ent in ents_copy:\n",
        "                # Skip self\n",
        "                if i != j:\n",
        "                    e_start, e_end, oe_start, oe_end = ent[0], ent[1], overlapping_ent[0], overlapping_ent[1]\n",
        "                    # Delete any ent that overlaps, keep if longer\n",
        "                    if ((e_start >= oe_start and e_start <= oe_end) \\\n",
        "                    or (e_end <= oe_end and e_end >= oe_start)) \\\n",
        "                    and ((e_end - e_start) <= (oe_end - oe_start)):\n",
        "                        ents.remove(ent)\n",
        "                j += 1\n",
        "            i += 1\n",
        "        clean_data.append((text, {'entities': ents}))\n",
        "                \n",
        "    return clean_data\n",
        "\n",
        "data = clean_ents(data)\n",
        "data[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoroSy-2KL0D"
      },
      "source": [
        "### TRAINING NER SPACY MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "zsoEdWfzYKbf"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import math\n",
        "\n",
        "#splitting the train and test set\n",
        "\n",
        "def train_test_split(data, test_size, random_state):\n",
        "\n",
        "    random.Random(random_state).shuffle(data)\n",
        "    test_idx = len(data) - math.floor(test_size * len(data))\n",
        "    train_set = data[0: test_idx]\n",
        "    test_set = data[test_idx: ]\n",
        "\n",
        "    return train_set, test_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "6AIrcxC0YiDB"
      },
      "outputs": [],
      "source": [
        "train_data, test_data = train_test_split(data, test_size = 0.1, random_state = 42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "GewG95UVYl_a"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import ast\n",
        "\n",
        "f = open(\"/content/Resume-parser-and-similarity-detection/skill.txt\", \"r\")\n",
        "l = f.read()\n",
        "patterns = l\n",
        "res = ast.literal_eval(patterns)\n",
        "\n",
        "def train_spacy():\n",
        "    \n",
        "    nlp = spacy.blank('en')  # create blank Language class\n",
        "    # create the built-in pipeline components and add them to the pipeline\n",
        "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
        "\n",
        "    # adding 'parser', 'ner','tagger' and 'entity_ruler' pipeline components\n",
        "    if 'ner' not in nlp.pipe_names and 'parser' not in nlp.pipe_names and 'tagger' not in nlp.pipe_names:\n",
        "\n",
        "        parser = nlp.create_pipe('parser')\n",
        "        nlp.add_pipe(parser, last = True)\n",
        "\n",
        "        tagger = nlp.create_pipe('tagger')\n",
        "        nlp.add_pipe(tagger, last = True)\n",
        "\n",
        "        ner = nlp.create_pipe('ner')\n",
        "        nlp.add_pipe(ner, last=True)\n",
        "\n",
        "        ruler = nlp.create_pipe('entity_ruler')\n",
        "        nlp.add_pipe(ruler, last = True)\n",
        "        ruler.add_patterns(res)\n",
        "        \n",
        "        \n",
        "    # add labels\n",
        "    for _, annotations in train_data:\n",
        "         for ent in annotations.get(\"entities\"):\n",
        "            ner.add_label(ent[2])\n",
        "            \n",
        "    # get names of other pipes to disable them during training\n",
        "    loss_arr = []\n",
        "    #other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
        "    #with nlp.disable_pipes(*other_pipes):  # only train NER\n",
        "    optimizer = nlp.begin_training()\n",
        "    for itn in range(20):\n",
        "        print(\"Starting iteration \" + str(itn))\n",
        "        random.shuffle(train_data)\n",
        "        losses = {}\n",
        "        for text, annotations in train_data:\n",
        "            nlp.update(\n",
        "                [text],  # batch of texts\n",
        "                [annotations],  # batch of annotations\n",
        "                drop=0.2,  # dropout - make it harder to memorise data\n",
        "                sgd=optimizer,  # callable to update weights\n",
        "                losses=losses)\n",
        "        loss_arr.append(losses[\"ner\"])\n",
        "        print(losses)\n",
        "    return nlp,loss_arr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNtnsvkJYrJy",
        "outputId": "41786d2a-2522-462c-93ea-5c4080012f06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting iteration 0\n",
            "{'ner': 27211.581893905695, 'parser': 0.0, 'tagger': 7024.516697672401}\n",
            "Starting iteration 1\n",
            "{'tagger': 1.4154521414038034e-15, 'ner': 20836.490112870415, 'parser': 0.0}\n",
            "Starting iteration 2\n",
            "{'parser': 0.0, 'ner': 15331.152762017673, 'tagger': 1.2761128564772292e-15}\n",
            "Starting iteration 3\n",
            "{'parser': 0.0, 'ner': 15207.400146322589, 'tagger': 1.3471601110007777e-15}\n",
            "Starting iteration 4\n",
            "{'tagger': 1.4294221672040701e-15, 'parser': 0.0, 'ner': 12543.739808729071}\n",
            "Starting iteration 5\n",
            "{'ner': 12489.308025791363, 'tagger': 1.3175154635999354e-15, 'parser': 0.0}\n",
            "Starting iteration 6\n",
            "{'ner': 10816.047314717522, 'parser': 0.0, 'tagger': 1.2179964731526127e-15}\n",
            "Starting iteration 7\n",
            "{'ner': 10193.267294404986, 'tagger': 1.2562667569083176e-15, 'parser': 0.0}\n",
            "Starting iteration 8\n",
            "{'ner': 10176.086674260812, 'tagger': 1.249470143769561e-15, 'parser': 0.0}\n",
            "Starting iteration 9\n",
            "{'ner': 8329.482171575952, 'tagger': 1.2541008526650493e-15, 'parser': 0.0}\n",
            "Starting iteration 10\n",
            "{'parser': 0.0, 'tagger': 1.308429992062483e-15, 'ner': 9253.783377666763}\n",
            "Starting iteration 11\n",
            "{'parser': 0.0, 'tagger': 1.2406677587313564e-15, 'ner': 8579.935027647536}\n",
            "Starting iteration 12\n",
            "{'parser': 0.0, 'tagger': 1.3434542583371477e-15, 'ner': 8158.401354687527}\n",
            "Starting iteration 13\n",
            "{'parser': 0.0, 'ner': 8626.025235292658, 'tagger': 1.2153875154894739e-15}\n",
            "Starting iteration 14\n",
            "{'parser': 0.0, 'ner': 8319.998875695208, 'tagger': 1.1442855125178187e-15}\n",
            "Starting iteration 15\n",
            "{'parser': 0.0, 'tagger': 1.1538475223408738e-15, 'ner': 6726.200886484337}\n",
            "Starting iteration 16\n",
            "{'tagger': 1.1346495479270894e-15, 'parser': 0.0, 'ner': 7177.658087336462}\n",
            "Starting iteration 17\n",
            "{'tagger': 1.191860269099727e-15, 'parser': 0.0, 'ner': 9037.693819242337}\n",
            "Starting iteration 18\n",
            "{'tagger': 1.1635434198246719e-15, 'ner': 6638.384623042014, 'parser': 0.0}\n",
            "Starting iteration 19\n",
            "{'tagger': 1.1178846802037863e-15, 'parser': 0.0, 'ner': 7582.45701084406}\n"
          ]
        }
      ],
      "source": [
        "nlp, loss_arr = train_spacy()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss_arr)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "loss_ar = np.array(loss_arr)\n",
        "\n",
        "x = np.arange(1, 21)\n",
        "y = loss_ar\n",
        " \n",
        "# plotting\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "figure(figsize=(8, 6), dpi=80)\n",
        "plt.title(\"NER Loss function\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.plot(x, y, color =\"red\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "id": "2pV48-Y0vOmU",
        "outputId": "a1754ed5-8a1d-4758-f926-13ada6ab1b1f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[27211.581893905695, 20836.490112870415, 15331.152762017673, 15207.400146322589, 12543.739808729071, 12489.308025791363, 10816.047314717522, 10193.267294404986, 10176.086674260812, 8329.482171575952, 9253.783377666763, 8579.935027647536, 8158.401354687527, 8626.025235292658, 8319.998875695208, 6726.200886484337, 7177.658087336462, 9037.693819242337, 6638.384623042014, 7582.45701084406]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "4WaFE5G6uxdD"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "DTnFT2b8s0h8"
      },
      "outputs": [],
      "source": [
        "#nlp.to_disk('/content/drive/My Drive/my_model')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "0rT4ykiIU67i"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "pickle_out = open(\"nlp.pkl\",\"wb\")\n",
        "pickle.dump(nlp,pickle_out)\n",
        "pickle_out.close()"
      ],
      "metadata": {
        "id": "uwdbDB0BU92z"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "WUCBlXCws05R"
      },
      "outputs": [],
      "source": [
        "# nlp = spacy.load('/content/drive/My Drive/my_model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apywOq04fPUU"
      },
      "source": [
        "# Finding accuracy of the model on the test set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "rop6LhDAY3b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b1a716e-7f36-420a-c3b8-0f44965d7dfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                       precision    recall  f1-score   support\n",
            "\n",
            "                    -       0.00      0.00      0.00       142\n",
            "       B-College Name       0.76      0.78      0.77        32\n",
            "       I-College Name       0.78      0.71      0.74        63\n",
            "       L-College Name       0.70      0.72      0.71        32\n",
            "       U-College Name       0.00      0.00      0.00         1\n",
            "B-Companies worked at       0.73      0.37      0.49        30\n",
            "I-Companies worked at       0.03      0.25      0.05         4\n",
            "L-Companies worked at       0.67      0.33      0.44        30\n",
            "U-Companies worked at       0.36      0.29      0.32        41\n",
            "             B-Degree       0.91      0.83      0.87        24\n",
            "             I-Degree       0.95      0.92      0.94        66\n",
            "             L-Degree       0.91      0.83      0.87        24\n",
            "             U-Degree       0.33      0.67      0.44         3\n",
            "        B-Designation       0.70      0.79      0.74        47\n",
            "        I-Designation       0.82      0.57      0.68        40\n",
            "        L-Designation       0.68      0.77      0.72        47\n",
            "        U-Designation       0.00      0.00      0.00         1\n",
            "      B-Email Address       1.00      1.00      1.00         7\n",
            "      I-Email Address       1.00      1.00      1.00        20\n",
            "      L-Email Address       1.00      1.00      1.00         7\n",
            "      U-Email Address       0.67      1.00      0.80        10\n",
            "    U-Graduation Year       0.43      0.14      0.21        22\n",
            "           B-Location       1.00      0.33      0.50         3\n",
            "           L-Location       1.00      0.33      0.50         3\n",
            "           U-Location       0.44      0.62      0.52        32\n",
            "               B-Name       0.91      0.91      0.91        23\n",
            "               L-Name       0.91      0.91      0.91        23\n",
            "                    O       0.95      0.92      0.94     12459\n",
            "             B-Skills       0.62      0.56      0.59        27\n",
            "             I-Skills       0.66      0.64      0.65      1022\n",
            "             L-Skills       0.62      0.56      0.59        27\n",
            "             U-Skills       0.00      0.00      0.00         2\n",
            "B-Years of Experience       0.00      0.00      0.00         4\n",
            "L-Years of Experience       0.00      0.00      0.00         4\n",
            "U-Years of Experience       0.00      0.00      0.00         1\n",
            "\n",
            "            micro avg       0.92      0.88      0.90     14323\n",
            "            macro avg       0.59      0.54      0.54     14323\n",
            "         weighted avg       0.91      0.88      0.90     14323\n",
            "          samples avg       0.88      0.88      0.88     14323\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from spacy.gold import GoldParse\n",
        "from itertools import groupby\n",
        "\n",
        "def doc_to_bilou(nlp, text):\n",
        "    \n",
        "    doc = nlp(text)\n",
        "    tokens = [(tok.text, tok.idx, tok.ent_type_) for tok in doc]\n",
        "    entities = []\n",
        "    for entity, group in groupby(tokens, key=lambda t: t[-1]):\n",
        "        if not entity:\n",
        "            continue\n",
        "        group = list(group)\n",
        "        _, start, _ = group[0]\n",
        "        word, last, _ = group[-1]\n",
        "        end = last + len(word)\n",
        "        \n",
        "        entities.append((\n",
        "                start,\n",
        "                end,\n",
        "                entity\n",
        "            ))\n",
        "\n",
        "    gold = GoldParse(nlp(text), entities = entities)\n",
        "    pred_ents = gold.ner\n",
        "    \n",
        "    return pred_ents\n",
        "\n",
        "y_test = []\n",
        "y_pred = []\n",
        "\n",
        "for text, annots in test_data:\n",
        "    \n",
        "    gold = GoldParse(nlp.make_doc(text), entities = annots.get(\"entities\"))\n",
        "    ents = gold.ner\n",
        "    pred_ents = doc_to_bilou(nlp, text)\n",
        "    \n",
        "    y_test.append(ents)\n",
        "    y_pred.append(pred_ents)\n",
        "    \n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from itertools import chain\n",
        "\n",
        "def ner_report(y_true, y_pred):\n",
        "\n",
        "    lb = LabelBinarizer()\n",
        "    y_true_combined = lb.fit_transform(list(chain.from_iterable(y_true)))\n",
        "    y_pred_combined = lb.transform(list(chain.from_iterable(y_pred)))\n",
        "        \n",
        "    tagset = set(lb.classes_)\n",
        "    tagset = sorted(tagset, key=lambda tag: tag.split('-', 1)[::-1])\n",
        "    class_indices = {cls: idx for idx, cls in enumerate(lb.classes_)}\n",
        "    \n",
        "    return classification_report(\n",
        "        y_true_combined,\n",
        "        y_pred_combined,\n",
        "        labels = [class_indices[cls] for cls in tagset],\n",
        "        target_names = tagset\n",
        "    ), accuracy_score(y_true_combined, y_pred_combined)\n",
        "    \n",
        "report, accuracy = ner_report(y_test, y_pred)\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "RWa9mJYim_u4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f8e53ef-a3c7-4db1-de8e-70d9c18a3b70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8814494170215736\n"
          ]
        }
      ],
      "source": [
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IScorKlam0ry"
      },
      "source": [
        "### Testing data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDF"
      ],
      "metadata": {
        "id": "k01jkNs7VDPj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d662952e-c4b4-4ba9-e509-f2a84a748935"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading PyMuPDF-1.19.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.8 MB 4.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: PyMuPDF\n",
            "Successfully installed PyMuPDF-1.19.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "reP7N3MFm4nG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "516a74c4-f6e9-486e-c66e-b3044ee1d810"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document('/content/Resume-parser-and-similarity-detection/KartikUllal_resume.pdf')"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "import sys, fitz\n",
        "fname = '/content/Resume-parser-and-similarity-detection/KartikUllal_resume.pdf'\n",
        "doc = fitz.open(fname)\n",
        "doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "mz7z-Zhqm4pn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "138099d3-e27d-4b1b-8f34-46082437110f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Kartik Ullal Boston, MA 02215 ∙ (857) 334-3460 kartikullal99@gmail.com ∙ www.linkedin.com/in/kartikullal ∙ Portfolio Website EDUCATION Master's of Science, Data Science Expected May 2023 Northeastern University, Khoury College of Computer Sciences, Boston CGPA: 3.78/4 ● Related Courses: Linear Algebra and probability for Data Science, Algorithms, Introduction to Data Management and Processing, Supervised Machine Learning, Natural Language Processing Bachelor's in Engineering, Information Technology June 2021 Mumbai University, Thadomal Shahani Engineering College(TSEC), Mumbai CGPA: 3.71/4 (9.18/10) ● Related Courses: Database Management Systems, Data Mining and Business Intelligence, Artificial Intelligence, Big Data Analytics, Object Oriented Programming  EXPERIENCE Web Developer Intern June 2020 - July 2020 Prayaas Corps, Jaipur, Rajasthan ● Developed a Covid-19 webpage to showcase work done for people during the Covid-19 crisis using HTML5, CSS3, and WordPress ● Collaborated with Social Media Manager to ensure alignment with the website and social media presence and increased views by 6% ● Taught 40 students mathematics in Prayaas ki Paathshaala(School) initiative Database Administrator Intern June 2019 - December 2019 Navlakhi Education, Mumbai, Maharashtra ● Designed database using MySql for JEE testing software and Mumbai Fashion Academy website ● Managed a team of six interns to efficiently implement and integrate the database using PHP ● Arranged five meetings with Mumbai Fashion Academy to gather schema requirements and built strong client relations  PROJECTS Predicting Depression, Stress, and Anxiety scores, Northeastern University September 2021 - December 2021 Github Link: DASS Prediction ● Coordinated with two engineers to build a machine learning model to predict the scores for depression, stress and anxiety from the DASS survey, using only personality traits and demographic data ● Preprocessed and cleaned the data to make it fit for prediction, and visualized different parameters to find high correlation with depression, stress and anxiety scores ● Implemented a StackCVRegressor Algorithm, that used predictions made by Lasso regression, Ridge regression, ElasticNet regression, Gradient Boosting regression, LightGBM and XGBoost regression as features to predict the scores, with an RMSE of 10. Comparative Study of the efficacy of Machine Learning Algorithms, TSEC August 2020 - May 2021 Github Link: Comparative Study Publication: Comparative Study ● Collaborated with two engineers to analyze LSTM, k-NN, Random Forest, SVR, and ARIMA models to compare the effectiveness for prediction on stock markets  over 3 datasets ● Utilized Python libraries, such as NumPy, Pandas, Scikit-Learn, and MatplotLib and evaluated each model by comparing the root mean squared error (RMSE) and mean absolute percentage error(MAPE) ● Examined the models, and concluded; ARIMA gave the best performance relative to other models with a 10% improvement in the RMSE and MAPE  Detection of Covid-19 using Chest X-rays, Hack India Crisis Hackathon March 2020 - April 2020 Github Link: Covid-19 Detection ● Cooperated with a team of four engineers to detect the presence of Covid-19 among patients with the classification abilities of Dense Convolutional Neural Networks (DenCOvseNets) on Chest X-rays ● Integrated, preprocessed and resized images of five different datasets to form a complete dataset of 17,194 samples of chest X-rays ● Trained the classification model with different train-validation split ratios and achieved an accuracy of 93.496% on the testing data TECHNICAL KNOWLEDGE Programming Languages: R, Python, SQL Skills: Numpy, Pandas, Scikit-learn, matplotlib, ggplot2, Tensorflow, Keras, Deep Learning, Machine Learning, Hadoop, Tableau, PowerBI, Pytorch, Natural Language Processing, Data Analytics, Data Visualization, SAS, Excel, NoSQL Certifications: Machine Learning by Stanford (Coursera) \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "text = ''\n",
        "for page in doc:\n",
        "    text = text + str(page.get_text())\n",
        "tx = \" \".join(text.split('\\n'))\n",
        "tx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "zmLV4vn_m4wm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7972ad47-b4e7-4b1f-8479-d41c5659fd7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kartik Ullal Boston, MA 02215 ∙ (857) 334-3460 kartikullal99@gmail.com ∙ www.linkedin.com/in/kartikullal ∙ Portfolio Website EDUCATION Master's of Science, Data Science Expected May 2023 Northeastern University, Khoury College of Computer Sciences, Boston CGPA: 3.78/4 ● Related Courses: Linear Algebra and probability for Data Science, Algorithms, Introduction to Data Management and Processing, Supervised Machine Learning, Natural Language Processing Bachelor's in Engineering, Information Technology June 2021 Mumbai University, Thadomal Shahani Engineering College(TSEC), Mumbai CGPA: 3.71/4 (9.18/10) ● Related Courses: Database Management Systems, Data Mining and Business Intelligence, Artificial Intelligence, Big Data Analytics, Object Oriented Programming  EXPERIENCE Web Developer Intern June 2020 - July 2020 Prayaas Corps, Jaipur, Rajasthan ● Developed a Covid-19 webpage to showcase work done for people during the Covid-19 crisis using HTML5, CSS3, and WordPress ● Collaborated with Social Media Manager to ensure alignment with the website and social media presence and increased views by 6% ● Taught 40 students mathematics in Prayaas ki Paathshaala(School) initiative Database Administrator Intern June 2019 - December 2019 Navlakhi Education, Mumbai, Maharashtra ● Designed database using MySql for JEE testing software and Mumbai Fashion Academy website ● Managed a team of six interns to efficiently implement and integrate the database using PHP ● Arranged five meetings with Mumbai Fashion Academy to gather schema requirements and built strong client relations  PROJECTS Predicting Depression, Stress, and Anxiety scores, Northeastern University September 2021 - December 2021 Github Link: DASS Prediction ● Coordinated with two engineers to build a machine learning model to predict the scores for depression, stress and anxiety from the DASS survey, using only personality traits and demographic data ● Preprocessed and cleaned the data to make it fit for prediction, and visualized different parameters to find high correlation with depression, stress and anxiety scores ● Implemented a StackCVRegressor Algorithm, that used predictions made by Lasso regression, Ridge regression, ElasticNet regression, Gradient Boosting regression, LightGBM and XGBoost regression as features to predict the scores, with an RMSE of 10. Comparative Study of the efficacy of Machine Learning Algorithms, TSEC August 2020 - May 2021 Github Link: Comparative Study Publication: Comparative Study ● Collaborated with two engineers to analyze LSTM, k-NN, Random Forest, SVR, and ARIMA models to compare the effectiveness for prediction on stock markets  over 3 datasets ● Utilized Python libraries, such as NumPy, Pandas, Scikit-Learn, and MatplotLib and evaluated each model by comparing the root mean squared error (RMSE) and mean absolute percentage error(MAPE) ● Examined the models, and concluded; ARIMA gave the best performance relative to other models with a 10% improvement in the RMSE and MAPE  Detection of Covid-19 using Chest X-rays, Hack India Crisis Hackathon March 2020 - April 2020 Github Link: Covid-19 Detection ● Cooperated with a team of four engineers to detect the presence of Covid-19 among patients with the classification abilities of Dense Convolutional Neural Networks (DenCOvseNets) on Chest X-rays ● Integrated, preprocessed and resized images of five different datasets to form a complete dataset of 17,194 samples of chest X-rays ● Trained the classification model with different train-validation split ratios and achieved an accuracy of 93.496% on the testing data TECHNICAL KNOWLEDGE Programming Languages: R, Python, SQL Skills: Numpy, Pandas, Scikit-learn, matplotlib, ggplot2, Tensorflow, Keras, Deep Learning, Machine Learning, Hadoop, Tableau, PowerBI, Pytorch, Natural Language Processing, Data Analytics, Data Visualization, SAS, Excel, NoSQL Certifications: Machine Learning by Stanford (Coursera) \n",
            "NAME                          - Kartik Ullal\n",
            "SKILL                         - Data Science\n",
            "COLLEGE NAME                  - Northeastern University\n",
            "SKILL                         - Data Science\n",
            "SKILL                         - Algorithms\n",
            "SKILL                         - Data Management\n",
            "SKILL                         - Machine Learning\n",
            "SKILL                         - Natural Language Processing\n",
            "SKILL                         - Engineering\n",
            "GRADUATION YEAR               - 2021\n",
            "LOCATION                      - Mumbai\n",
            "SKILL                         - Engineering\n",
            "SKILL                         - Database\n",
            "SKILL                         - Data Mining\n",
            "SKILL                         - Business Intelligence\n",
            "SKILL                         - Artificial Intelligence\n",
            "SKILL                         - Big Data\n",
            "SKILL                         - Analytics\n",
            "SKILL                         - HTML5\n",
            "SKILL                         - WordPress\n",
            "SKILL                         - Database\n",
            "SKILL                         - database\n",
            "SKILL                         - MySql\n",
            "SKILL                         - testing\n",
            "SKILL                         - software\n",
            "SKILL                         - database\n",
            "SKILL                         - PHP\n",
            "SKILL                         - Github\n",
            "SKILL                         - machine learning\n",
            "SKILL                         - Algorithm\n",
            "SKILL                         - Machine Learning\n",
            "SKILL                         - Algorithms\n",
            "SKILL                         - Github\n",
            "SKILL                         - Random Forest\n",
            "SKILL                         - Python\n",
            "SKILL                         - libraries\n",
            "SKILL                         - NumPy\n",
            "SKILL                         - Pandas\n",
            "SKILL                         - Github\n",
            "SKILL                         - testing\n",
            "SKILL                         - Languages\n",
            "SKILL                         - R\n",
            "SKILL                         - Python\n",
            "SKILL                         - SQL\n",
            "SKILL                         - Numpy\n",
            "SKILL                         - Pandas\n",
            "SKILL                         - Tensorflow\n",
            "SKILL                         - Keras\n",
            "SKILL                         - Deep Learning\n",
            "SKILL                         - Machine Learning\n",
            "SKILL                         - Hadoop\n",
            "SKILL                         - Tableau\n",
            "SKILL                         - Pytorch\n",
            "SKILL                         - Natural Language Processing\n",
            "SKILL                         - Analytics\n",
            "SKILL                         - Data Visualization\n",
            "SKILL                         - NoSQL\n",
            "SKILL                         - Machine Learning\n"
          ]
        }
      ],
      "source": [
        "doc = nlp(tx)\n",
        "print(doc)\n",
        "for ent in doc.ents:\n",
        "    print(f'{ent.label_.upper():{30}}- {ent.text}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nENzKvKmL1q"
      },
      "source": [
        "### Extract Skills\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "Reo1bUHvga6C"
      },
      "outputs": [],
      "source": [
        "def extract_spacy():\n",
        "    \n",
        "    nlp1 = spacy.blank('en')  # create blank Language class\n",
        "    # create the built-in pipeline components and add them to the pipeline\n",
        "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
        "    if 'entity_ruler' not in nlp1.pipe_names:\n",
        "\n",
        "        ruler = nlp1.create_pipe('entity_ruler')\n",
        "        nlp1.add_pipe(ruler, last = True)\n",
        "        ruler.add_patterns(res) \n",
        "\n",
        "    return nlp1\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "kyy79mTjKTqX"
      },
      "outputs": [],
      "source": [
        "extract = extract_spacy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "Tn6_zG__NzBm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f71cbeb-0e99-4169-967a-ff6926185359"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kartik Ullal Boston, MA 02215 ∙ (857) 334-3460 kartikullal99@gmail.com ∙ www.linkedin.com/in/kartikullal ∙ Portfolio Website EDUCATION Master's of Science, Data Science Expected May 2023 Northeastern University, Khoury College of Computer Sciences, Boston CGPA: 3.78/4 ● Related Courses: Linear Algebra and probability for Data Science, Algorithms, Introduction to Data Management and Processing, Supervised Machine Learning, Natural Language Processing Bachelor's in Engineering, Information Technology June 2021 Mumbai University, Thadomal Shahani Engineering College(TSEC), Mumbai CGPA: 3.71/4 (9.18/10) ● Related Courses: Database Management Systems, Data Mining and Business Intelligence, Artificial Intelligence, Big Data Analytics, Object Oriented Programming  EXPERIENCE Web Developer Intern June 2020 - July 2020 Prayaas Corps, Jaipur, Rajasthan ● Developed a Covid-19 webpage to showcase work done for people during the Covid-19 crisis using HTML5, CSS3, and WordPress ● Collaborated with Social Media Manager to ensure alignment with the website and social media presence and increased views by 6% ● Taught 40 students mathematics in Prayaas ki Paathshaala(School) initiative Database Administrator Intern June 2019 - December 2019 Navlakhi Education, Mumbai, Maharashtra ● Designed database using MySql for JEE testing software and Mumbai Fashion Academy website ● Managed a team of six interns to efficiently implement and integrate the database using PHP ● Arranged five meetings with Mumbai Fashion Academy to gather schema requirements and built strong client relations  PROJECTS Predicting Depression, Stress, and Anxiety scores, Northeastern University September 2021 - December 2021 Github Link: DASS Prediction ● Coordinated with two engineers to build a machine learning model to predict the scores for depression, stress and anxiety from the DASS survey, using only personality traits and demographic data ● Preprocessed and cleaned the data to make it fit for prediction, and visualized different parameters to find high correlation with depression, stress and anxiety scores ● Implemented a StackCVRegressor Algorithm, that used predictions made by Lasso regression, Ridge regression, ElasticNet regression, Gradient Boosting regression, LightGBM and XGBoost regression as features to predict the scores, with an RMSE of 10. Comparative Study of the efficacy of Machine Learning Algorithms, TSEC August 2020 - May 2021 Github Link: Comparative Study Publication: Comparative Study ● Collaborated with two engineers to analyze LSTM, k-NN, Random Forest, SVR, and ARIMA models to compare the effectiveness for prediction on stock markets  over 3 datasets ● Utilized Python libraries, such as NumPy, Pandas, Scikit-Learn, and MatplotLib and evaluated each model by comparing the root mean squared error (RMSE) and mean absolute percentage error(MAPE) ● Examined the models, and concluded; ARIMA gave the best performance relative to other models with a 10% improvement in the RMSE and MAPE  Detection of Covid-19 using Chest X-rays, Hack India Crisis Hackathon March 2020 - April 2020 Github Link: Covid-19 Detection ● Cooperated with a team of four engineers to detect the presence of Covid-19 among patients with the classification abilities of Dense Convolutional Neural Networks (DenCOvseNets) on Chest X-rays ● Integrated, preprocessed and resized images of five different datasets to form a complete dataset of 17,194 samples of chest X-rays ● Trained the classification model with different train-validation split ratios and achieved an accuracy of 93.496% on the testing data TECHNICAL KNOWLEDGE Programming Languages: R, Python, SQL Skills: Numpy, Pandas, Scikit-learn, matplotlib, ggplot2, Tensorflow, Keras, Deep Learning, Machine Learning, Hadoop, Tableau, PowerBI, Pytorch, Natural Language Processing, Data Analytics, Data Visualization, SAS, Excel, NoSQL Certifications: Machine Learning by Stanford (Coursera) \n",
            "SKILL                         - Data Science\n",
            "SKILL                         - Data Science\n",
            "SKILL                         - Algorithms\n",
            "SKILL                         - Data Management\n",
            "SKILL                         - Machine Learning\n",
            "SKILL                         - Natural Language Processing\n",
            "SKILL                         - Engineering\n",
            "SKILL                         - Engineering\n",
            "SKILL                         - Database\n",
            "SKILL                         - Data Mining\n",
            "SKILL                         - Business Intelligence\n",
            "SKILL                         - Artificial Intelligence\n",
            "SKILL                         - Big Data\n",
            "SKILL                         - Analytics\n",
            "SKILL                         - HTML5\n",
            "SKILL                         - WordPress\n",
            "SKILL                         - Database\n",
            "SKILL                         - database\n",
            "SKILL                         - MySql\n",
            "SKILL                         - testing\n",
            "SKILL                         - software\n",
            "SKILL                         - database\n",
            "SKILL                         - PHP\n",
            "SKILL                         - Github\n",
            "SKILL                         - machine learning\n",
            "SKILL                         - Algorithm\n",
            "SKILL                         - Machine Learning\n",
            "SKILL                         - Algorithms\n",
            "SKILL                         - Github\n",
            "SKILL                         - Random Forest\n",
            "SKILL                         - Python\n",
            "SKILL                         - libraries\n",
            "SKILL                         - NumPy\n",
            "SKILL                         - Pandas\n",
            "SKILL                         - Github\n",
            "SKILL                         - testing\n",
            "SKILL                         - Languages\n",
            "SKILL                         - R\n",
            "SKILL                         - Python\n",
            "SKILL                         - SQL\n",
            "SKILL                         - Numpy\n",
            "SKILL                         - Pandas\n",
            "SKILL                         - Tensorflow\n",
            "SKILL                         - Keras\n",
            "SKILL                         - Deep Learning\n",
            "SKILL                         - Machine Learning\n",
            "SKILL                         - Hadoop\n",
            "SKILL                         - Tableau\n",
            "SKILL                         - Pytorch\n",
            "SKILL                         - Natural Language Processing\n",
            "SKILL                         - Analytics\n",
            "SKILL                         - Data Visualization\n",
            "SKILL                         - NoSQL\n",
            "SKILL                         - Machine Learning\n"
          ]
        }
      ],
      "source": [
        "doc = extract(tx)\n",
        "print(doc)\n",
        "for ent in doc.ents:\n",
        "    print(f'{ent.label_.upper():{30}}- {ent.text}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XJ9LhZarr8w"
      },
      "source": [
        "### Job Description Similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "DgFS9m3BsDJB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4e62a74-d1c5-4834-b4aa-0b5fed8034a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import gensim\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "nHwsPcHDsvSY"
      },
      "outputs": [],
      "source": [
        "\n",
        "# This is a function to extract skills from the resume-parser dataset to train Doc2Vec model\n",
        "def get_skills(data):\n",
        "    temp_skill = []\n",
        "    #generate a 2-D list skills and name from the resume\n",
        "    for j in data:\n",
        "        if j['label'] == ['Skills']:\n",
        "            temp_skill.append(j['points'][0]['text'])\n",
        "        if j['label'] == ['Name']:\n",
        "            temp_name = j['points'][0]['text']\n",
        "        else:\n",
        "            temp_name = \"No name\"\n",
        "\n",
        "    #clean the list of skills\n",
        "    for i,j in enumerate(temp_skill):\n",
        "        j = j.replace(\"•\",\"\")\n",
        "        j = j.replace('\\n',\",\")\n",
        "        j = re.sub(\"[^A-Za-z0-9+-, ]\",\"\",j)\n",
        "        j = j.split(',')\n",
        "        j = [x for x in j if x!= '']\n",
        "        temp_skill[i] = j\n",
        "        \n",
        "    temp_s = []\n",
        "\n",
        "    #Convert the 2-D list into a 1-D list\n",
        "    for j in temp_skill:\n",
        "        for i in j:\n",
        "            temp_s.append(i)\n",
        "\n",
        "    \n",
        "    return (temp_name, temp_s)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "sdaa8ke7swwF"
      },
      "outputs": [],
      "source": [
        "skills = {}\n",
        "df = pd.read_json('/content/Resume-parser-and-similarity-detection/Entity Recognition in Resumes.json', lines = True)\n",
        "data = df[\"annotation\"]\n",
        "#Create a dictionary of skills with key as name and value as the list of skills\n",
        "for i in data:\n",
        "    name,skill = get_skills(i)\n",
        "    skills[name] = skill\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40tqX6vwsQcp"
      },
      "source": [
        "Cosine Similarity function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "SeFKNJdnrSVh"
      },
      "outputs": [],
      "source": [
        "#This is a function to calculate cosine similarity between 2 vectors\n",
        "def cosine(u, v):\n",
        "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9jJ2XCrsXqX"
      },
      "source": [
        "Doc2Vec\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "GaGG7mWTsWUL"
      },
      "outputs": [],
      "source": [
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "\n",
        "# This is a function to train the doc2vec model using the skills dictionary\n",
        "def doc2vec_similarity_train(skills):\n",
        "    tokenized_dict = {}\n",
        "\n",
        "    # Here we create a tokenized dictionary with key as name and values as tokens from the skills\n",
        "    for n,s in skills.items():\n",
        "        tokenized_list = []\n",
        "        for i in s:\n",
        "            x = word_tokenize(i.lower())\n",
        "            for j in x:\n",
        "                tokenized_list.append(j)\n",
        "        tokenized_dict[n] = tokenized_list\n",
        "    \n",
        "    # This tags each document (tokens of skills) to feed to the Doc2Vec model as input\n",
        "    tagged_data = [TaggedDocument(d, [i]) for i, d in enumerate(tokenized_dict.values())]\n",
        "\n",
        "    # we create the doc2Vec model that outputs a vector of length 40. \n",
        "    # Window size = 3 for the continours bag of words\n",
        "    # We don't count words with count less than 1 \n",
        "    # and we train for 100 epochs\n",
        "    model = Doc2Vec(vector_size=40,window = 3, min_count = 1, epochs = 100)\n",
        "    \n",
        "    #build and train the model\n",
        "    model.build_vocab(tagged_data)\n",
        "    model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
        "    \n",
        "    return model\n",
        "\n",
        "#this is a function to test the job requirement and resume skills and output similarity\n",
        "def doc2vec_similarity_test(model, req, skill):\n",
        "    tokenized_req = []\n",
        "    tokenized_skill = []\n",
        "    #tokenizes the job requirements\n",
        "    for i in req:\n",
        "        x = word_tokenize(i.lower())\n",
        "        for j in x:\n",
        "                tokenized_req.append(j)\n",
        "    #tokenizes the resume skills\n",
        "    for i in skill:\n",
        "        x = word_tokenize(i.lower())\n",
        "        for j in x:\n",
        "                tokenized_skill.append(j)\n",
        "    \n",
        "    #calculates vector for resume skills\n",
        "    skill_vector = model.infer_vector(tokenized_skill)\n",
        "    #calculates vector for job requirement skills\n",
        "    req_vector = model.infer_vector(tokenized_req)\n",
        "    \n",
        "    #calculates similarity\n",
        "    similarity = cosine(skill_vector, req_vector)\n",
        "    \n",
        "    return similarity\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "GWO-yBdMsa1O"
      },
      "outputs": [],
      "source": [
        "#train the model with the skills dictionary\n",
        "model = doc2vec_similarity_train(skills)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mMYUoFiuRk4"
      },
      "source": [
        "Sentence Transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "UEHstjcmuT4i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0b03821-6e55-488c-f1cc-071d19313a01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.7/dist-packages (2.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.64.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.1.96)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.5.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.11.1+cu111)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.18.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (4.1.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (4.11.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.12.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.0.49)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.8.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "pip install -U sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "SFZaFCnHtWj7"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "#this is a function to calculate similarity using sentence-bert\n",
        "\n",
        "def sentence_bert_similarity(req, skill):\n",
        "    #Here we use and import pre trained sentence bert model \n",
        "    sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "    \n",
        "    # here we are converting the skills into a single string\n",
        "    req = \",\".join(req)\n",
        "    skill = \",\".join(skill)\n",
        "    \n",
        "    #calculating the vectors for resume and job description\n",
        "    skill_vec = sbert_model.encode([skill])[0]\n",
        "    req_vec = sbert_model.encode([req])[0]\n",
        "    \n",
        "    #calculate similarity\n",
        "    similarity = cosine(skill_vec, req_vec)\n",
        "    \n",
        "    return similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "RkXGqlCQsIjQ"
      },
      "outputs": [],
      "source": [
        "#get user skills from trained NER model\n",
        "user_skills = [i.text for i in doc.ents]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#function to clean and preprocess job description \n",
        "def clean_job(description):\n",
        "  description = description.lower()\n",
        "  description= description.replace(\"•\",\" \")\n",
        "  description = description.replace('\\n',\",\")\n",
        "  description = re.sub(\"[^A-Za-z0-9,]\",\" \",description)\n",
        "\n",
        "  return description\n",
        "\n"
      ],
      "metadata": {
        "id": "4DRdr6PaK1dZ"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Science Job description"
      ],
      "metadata": {
        "id": "h4EVXi6jKOgf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "ElB8-DNiJwMs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e15d9c28-435d-482c-b74a-e2669c2501c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "to qualify you must have a 1  masters degree in a quantitative discipline  biomedical informatics, computer science, machine learning, applied statistics, mathematics or similar field, proficiency in at least one programming language  python, r  and machine learning tools  scikit learn, r , knowledge of predictive modeling and machine learning concepts, including design, development, evaluation, deployment and scaling to large datasets, familiarity with computing models for big data hadoop   mapreduce, spark etc , knowledge of databases  relational   sql, nosql, mongodb, etc  , good grasp of software engineering principles  experience in integrating modern software architectures, knowledge and some experience in operational aspects of software development and deployment, including automation, testing, virtualization and container technology, knowledge of clinical and operational aspects of healthcare delivery, excellent written and oral communication skills for a variety of audiences, preferred qualifications, phd degree in a quantitative field  biomedical informatics, computer science, machine learning, applied statistics, mathematics or similar field    2 years experience, demonstrated skills in design and implementation of complex machine learning models, demonstrated knowledge of software engineering and operational skills through prior projects \n",
            "SKILL                         - computer science\n",
            "SKILL                         - machine learning\n",
            "SKILL                         - programming language\n",
            "SKILL                         - python\n",
            "SKILL                         - machine learning tools\n",
            "SKILL                         - scikit learn\n",
            "SKILL                         - machine learning\n",
            "SKILL                         - design\n",
            "SKILL                         - deployment\n",
            "SKILL                         - big data\n",
            "SKILL                         - hadoop\n",
            "SKILL                         - databases\n",
            "SKILL                         - nosql\n",
            "SKILL                         - mongodb\n",
            "SKILL                         - software engineering\n",
            "SKILL                         - software\n",
            "SKILL                         - software\n",
            "SKILL                         - deployment\n",
            "SKILL                         - testing\n",
            "SKILL                         - computer science\n",
            "SKILL                         - machine learning\n",
            "SKILL                         - design\n",
            "SKILL                         - machine learning\n",
            "SKILL                         - software engineering\n",
            "0.89131653\n",
            "0.8546861\n"
          ]
        }
      ],
      "source": [
        "#job description for data science role\n",
        "job_req = \"To qualify you must have a 1. Masters degree in a quantitative discipline (Biomedical Informatics, Computer Science, Machine Learning, Applied Statistics, Mathematics or similar field, Proficiency in at least one programming language (Python, R) and machine learning tools (scikit learn, R), Knowledge of predictive modeling and machine learning concepts, including design, development, evaluation, deployment and scaling to large datasets, Familiarity with computing models for big data Hadoop / MapReduce, Spark etc., Knowledge of databases (Relational / SQL, NOSQL, MongoDB, etc.), Good grasp of software engineering principles. Experience in integrating modern software architectures, Knowledge and some experience in operational aspects of software development and deployment, including automation, testing, virtualization and container technology, Knowledge of clinical and operational aspects of healthcare delivery, Excellent written and oral communication skills for a variety of audiences, Preferred Qualifications, PhD degree in a quantitative field (Biomedical Informatics, Computer Science, Machine Learning, Applied Statistics, Mathematics or similar field) + 2 years experience, Demonstrated skills in design and implementation of complex machine learning models, Demonstrated knowledge of software engineering and operational skills through prior projects.\"\n",
        "\n",
        "job_req = clean_job(job_req)\n",
        " \n",
        "print(job_req)\n",
        "\n",
        "# run the ner model on the job description and extract the skills\n",
        "doc = extract(job_req)\n",
        "for ent in doc.ents:\n",
        "    print(f'{ent.label_.upper():{30}}- {ent.text}')\n",
        "\n",
        "job_skills = [i.text for i in doc.ents]\n",
        "\n",
        "\n",
        "#similarity using sentence_bert\n",
        "print(sentence_bert_similarity(job_skills, user_skills))\n",
        "\n",
        "#similarity using doc2vec\n",
        "print(doc2vec_similarity_test(model, job_skills,user_skills))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Software Engineering Job Description\n"
      ],
      "metadata": {
        "id": "bAvyCyKKKUev"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "KYXEseasKawB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3502e9f-871f-4783-d143-8ef07309a795"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "computer science, engineering or related degree with a minimum gpa of 3 0 or higher software engineering skills and experience with some of the following technologies  java,  net, node js, python, angular, react, aws, azure, gcp, sql or mobile knowledge of common data structures and algorithmsstrong problem solving and software triage skills with the ability to work cross functionally in a fast paced and rapidly changing work environment strong analytical and interpersonal communication skills\n",
            "SKILL                         - computer science\n",
            "SKILL                         - engineering\n",
            "SKILL                         - software engineering\n",
            "SKILL                         - java\n",
            "SKILL                         - node js\n",
            "SKILL                         - python\n",
            "SKILL                         - angular\n",
            "SKILL                         - react\n",
            "SKILL                         - azure\n",
            "SKILL                         - mobile\n",
            "SKILL                         - data structures\n",
            "SKILL                         - software\n",
            "0.8336387\n",
            "0.49812713\n"
          ]
        }
      ],
      "source": [
        "#job description for software engineer role\n",
        "job_req = \"Computer Science, Engineering or related degree with a minimum GPA of 3.0 or higher Software engineering skills and experience with some of the following technologies: Java, .NET, Node.js, Python, Angular, React, AWS, Azure, GCP, SQL or mobile Knowledge of common data structures and algorithmsStrong problem-solving and software triage skills with the ability to work cross-functionally in a fast-paced and rapidly changing work environment Strong analytical and interpersonal communication skills\"\n",
        "\n",
        "job_req = clean_job(job_req)\n",
        "\n",
        "print(job_req)\n",
        "\n",
        "# run the ner model on the job description and extract the skills\n",
        "doc = extract(job_req)\n",
        "for ent in doc.ents:\n",
        "    print(f'{ent.label_.upper():{30}}- {ent.text}')\n",
        "\n",
        "job_skills = [i.text for i in doc.ents]\n",
        "\n",
        "#similarity using sentence_bert\n",
        "print(sentence_bert_similarity(job_skills, user_skills))\n",
        "\n",
        "#similarity using doc2vec\n",
        "print(doc2vec_similarity_test(model, job_skills,user_skills))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "SALhgU-mc9ot"
      },
      "execution_count": 66,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Resume_NER_Parsing_1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}